{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de45a8c6",
   "metadata": {},
   "source": [
    "## Scikit-learn Preprocessing — Theory-Only "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e067cde",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What you MUST learn from this section\n",
    "Learn **concepts + rules**, not formulas or toy examples.\n",
    "\n",
    "### Core ideas to keep\n",
    "- Scaling exists to **stabilize optimization**, not to “improve accuracy magically”\n",
    "- Different data → different scaler\n",
    "- Fit scaler on **train only**\n",
    "- Pipelines exist to prevent **data leakage**\n",
    "\n",
    "Everything else is secondary.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Standardization (StandardScaler) — ACTUALLY IMPORTANT\n",
    "\n",
    "### What it really does\n",
    "- Subtracts mean (centering)\n",
    "- Divides by standard deviation (scaling)\n",
    "\n",
    "### When you MUST use it\n",
    "- Linear Regression\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- PCA\n",
    "- Any model with:\n",
    "  - Regularization\n",
    "  - Distance / dot products\n",
    "\n",
    "### When you MUST NOT care\n",
    "- Tree-based models (Decision Tree, RF, GBM)\n",
    "- Rule-based models\n",
    "\n",
    "### What to ignore\n",
    "- “Gaussian” wording  \n",
    "  → Model **does not require** normal distribution  \n",
    "- Exact numeric example  \n",
    "  → Just illustration\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Term: **Sparsity** (important, not optional)\n",
    "\n",
    "### What sparsity means\n",
    "- Data with **many zeros**\n",
    "- Example:\n",
    "\n",
    "\n",
    "### Where sparse data comes from\n",
    "- One-hot encoding\n",
    "- Bag-of-words / TF-IDF\n",
    "- High-dimensional categorical data\n",
    "\n",
    "### Why it matters\n",
    "- Sparse matrices store **only non-zero values**\n",
    "- Efficient memory + speed\n",
    "\n",
    "### Why centering breaks sparsity\n",
    "- Centering adds mean to zeros\n",
    "- Zeros → non-zero\n",
    "- Memory explodes\n",
    "\n",
    "**Rule**\n",
    "> Never center sparse data\n",
    "\n",
    "---\n",
    "\n",
    "## 4. MinMaxScaler — When it makes sense\n",
    "\n",
    "### What it does\n",
    "- Scales features to fixed range (usually 0–1)\n",
    "\n",
    "### Use it when\n",
    "- You need bounded values\n",
    "- Feature std is tiny or unstable\n",
    "- Some NN setups\n",
    "\n",
    "### DO NOT use when\n",
    "- Strong outliers exist\n",
    "- Data distribution can shift a lot\n",
    "\n",
    "### Ignore\n",
    "- Formula derivation\n",
    "- `min_`, `scale_` attributes unless debugging\n",
    "\n",
    "---\n",
    "\n",
    "## 5. MaxAbsScaler — Why it exists\n",
    "\n",
    "### What it does\n",
    "- Divides by max absolute value\n",
    "- Output in [-1, 1]\n",
    "\n",
    "### Why it’s special\n",
    "- Does **not center**\n",
    "- Preserves zeros\n",
    "\n",
    "### When to use\n",
    "- Sparse data\n",
    "- Features already centered near zero\n",
    "\n",
    "**Mental rule**\n",
    "> Sparse data → MaxAbsScaler\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Scaling sparse data — non-negotiable rules\n",
    "\n",
    "### Allowed\n",
    "- MaxAbsScaler\n",
    "- StandardScaler(with_mean=False)\n",
    "\n",
    "### Forbidden\n",
    "- StandardScaler() default\n",
    "- RobustScaler.fit() on sparse data\n",
    "\n",
    "### Why sklearn yells at you\n",
    "- Silent centering = RAM explosion\n",
    "- Better to crash than corrupt memory\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Outliers — what matters\n",
    "\n",
    "### What is an outlier\n",
    "- Extreme value far from majority\n",
    "- Skews mean and std\n",
    "\n",
    "### Problem\n",
    "- StandardScaler gets distorted\n",
    "\n",
    "### Solution\n",
    "- RobustScaler\n",
    "- Uses median + IQR\n",
    "- Resistant to outliers\n",
    "\n",
    "### Tradeoff\n",
    "- Less sensitive\n",
    "- Slightly slower\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Kernel centering — SKIP THIS\n",
    "\n",
    "### Only relevant if\n",
    "- You manually compute kernel matrices\n",
    "- You know what a Gram matrix is\n",
    "- You are deep into kernel theory\n",
    "\n",
    "### For you\n",
    "- **Ignore completely**\n",
    "\n",
    "---\n",
    "\n",
    "## 9. What to IGNORE safely\n",
    "- Toy numeric arrays\n",
    "- Exact formulas\n",
    "- Attribute introspection examples\n",
    "- KernelCenterer section\n",
    "- CSR vs CSC details (for now)\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Final mental map (remember this)\n",
    "- Linear models → scale\n",
    "- Tree models → don’t care\n",
    "- Sparse data → never center\n",
    "- Outliers → RobustScaler\n",
    "- Pipelines → no leakage\n",
    "- Scaling fixes **optimization**, not **data quality**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c37d65",
   "metadata": {},
   "source": [
    "## 7.3.1 Standardization, or mean removal and variance scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46c3ac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.         0.33333333]\n",
      "\n",
      "[0.81649658 0.81649658 1.24721913]\n",
      "\n",
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n",
      "\n",
      "[0. 0. 0.]\n",
      "\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np \n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "print (scaler.mean_)\n",
    "print()\n",
    "\n",
    "print (scaler.scale_)\n",
    "print()\n",
    "\n",
    "X_scaled = scaler.transform(X_train)\n",
    "print (X_scaled)\n",
    "\n",
    "print()\n",
    "print(X_scaled.mean(axis=0))\n",
    "\n",
    "print()\n",
    "print(X_scaled.std(axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05a334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_classification(random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe.fit(X_train, y_train) # apply scaling on training data\n",
    "pipe.score(X_test,y_test)  # apply scaling on testing data, without leaking training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba19a6",
   "metadata": {},
   "source": [
    "### 7.3.1.1. Scaling features to a range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56afb730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 1.        ],\n",
       "       [1.        , 0.5       , 0.33333333],\n",
       "       [0.        , 1.        , 0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d88434b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5       ,  0.        ,  1.66666667]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array([[-3., -1.,  4.]])\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d670120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.5       , 0.33333333])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba5fcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.5       , 0.33333333])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "757f30a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 2.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaler.fit_transform(X_train)\n",
    "X_train_maxabs\n",
    "X_test = np.array([[ -3., -1.,  4.]])\n",
    "X_test_maxabs = max_abs_scaler.transform(X_test)\n",
    "X_test_maxabs\n",
    "max_abs_scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eda77c",
   "metadata": {},
   "source": [
    "## 7.3.4. Encoding categorical features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MrBlink",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
